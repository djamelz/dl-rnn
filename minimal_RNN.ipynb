{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal RNN notebook\n",
    "\n",
    "### RNN representation\n",
    "\n",
    "Contrary to a Feed Forward Neural Network, an RNN is a recurrent neural network, in which the information flow is not linear. A general representation can be seen as follows:\n",
    "\n",
    "![Representation](img/rnn_simple.svg)\n",
    "\n",
    "An RNN is useful to deal with sequential information: a sequence of inputs is fed through the network and the hidden state is updated at each step of the sequence. The sequence is commonly represented as a time sequence, and the most straight forward learning algorithm is backpropagation through time (BPTT) http://en.wikipedia.org/wiki/Backpropagation_through_time.\n",
    "\n",
    "To understand properly BPTT, a better representation of the RNN is its unfolded version:\n",
    "\n",
    "![Representation](img/rnn_unfolded.svg)\n",
    "\n",
    "The input X is a sequence $x_0, x_1, ... x_t$, at each time-step t a new input $x_t$ is fed to the network.\n",
    "\n",
    "### Equations\n",
    "\n",
    "The most simple forward equations for a RNN are as follows:\n",
    "\n",
    "$$h_t = \\tanh(x_t . W_{in} + h_{t-1} . W_{rec})$$\n",
    "$$y_t = softmax(h_t . W_{out})$$\n",
    "\n",
    "Depending on the problem, all the outputs $y_0, ... y_t$ might be useful, or just $y_t$ the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import shared \n",
    "from collections import OrderedDict\n",
    "\n",
    "dtype=T.config.floatX\n",
    "theano.config.optimizer='fast_compile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(shape, name, sample='uni'):\n",
    "    if sample=='unishape':\n",
    "        return shared(value=np.asarray(np.random.uniform(\n",
    "                low=-np.sqrt(6. / (shape[0] + shape[1])),\n",
    "                high=np.sqrt(6. / (shape[0] + shape[1])),\n",
    "                size=shape), dtype=dtype), \n",
    "                    name=name, borrow=True)\n",
    "    \n",
    "    if sample=='svd':\n",
    "        values = np.ndarray(shape, dtype=dtype)\n",
    "        for dx in xrange(shape[0]):\n",
    "            vals = np.random.uniform(low=-1., high=1.,  size=(shape[1],))\n",
    "            values[dx,:] = vals\n",
    "        _,svs,_ = np.linalg.svd(values)\n",
    "        #svs[0] is the largest singular value                      \n",
    "        values = values / svs[0]\n",
    "        return shared(values, name=name, borrow=True)\n",
    "    \n",
    "    if sample=='uni':\n",
    "        return shared(value=np.asarray(np.random.uniform(low=-0.1,high=0.1, size=shape), dtype=dtype), \n",
    "                      name=name, borrow=True)\n",
    "    \n",
    "    if sample=='zero':\n",
    "        return shared(value=np.zeros(shape=shape, dtype=dtype), \n",
    "                      name=name, borrow=True)\n",
    "    \n",
    "    \n",
    "    raise \"error bad sample technique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Rnn:\n",
    "    def __init__(self, n_in, n_hid, n_out, lr):   \n",
    "        self.n_in = n_in\n",
    "        self.n_hid = n_hid\n",
    "        self.n_out = n_out\n",
    "        self.W_in = init_weight((self.n_in, self.n_hid),'W_in', 'svd')\n",
    "        self.W_out = init_weight((self.n_hid, self.n_out),'W_out', 'svd')\n",
    "        self.W_rec = init_weight((self.n_hid, self.n_hid),'W_rec', 'svd')\n",
    "        self.b_out = init_weight((self.n_out), 'b_out','zero')\n",
    "        self.params = [self.W_in,self.W_out,self.W_rec, self.b_out]\n",
    "        \n",
    "        def step(x_t, h_tm1):\n",
    "            h_t = T.tanh(T.dot(x_t, self.W_in) + T.dot(h_tm1, self.W_rec))\n",
    "            y_t = T.nnet.softmax(- (T.dot(h_t, self.W_out) + self.b_out))            \n",
    "            return [h_t, y_t]\n",
    "\n",
    "        X = T.matrix() # X is a sequence of vectors\n",
    "        Y = T.matrix() # Y is a sequence of vectors\n",
    "        h0 = shared(np.zeros(self.n_hid, dtype=dtype)) # initial hidden state         \n",
    "        lr = shared(np.cast[dtype](lr))\n",
    "        \n",
    "        [h_vals, y_vals], _ = theano.scan(fn=step,                                  \n",
    "                                          sequences=X,\n",
    "                                          outputs_info=[h0, None])\n",
    "        \n",
    "        #h_vals is a sequence of hidden states\n",
    "        #y_vals is a sequence of outputs\n",
    "        \n",
    "        # compute cost : cross entropy cost\n",
    "        cost = -T.mean(Y * T.log(y_vals)+ (1.- Y) * T.log(1. - y_vals))        \n",
    "        # for mean squared error, use \n",
    "        # cost = -T.mean((Y - y_vals) ** 2)\n",
    "        \n",
    "        gparams = T.grad(cost, self.params)\n",
    "        updates = OrderedDict()\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates[param] = param - gparam * lr\n",
    "                \n",
    "        self.train = theano.function(inputs = [X, Y], outputs = cost, updates=updates)\n",
    "        self.predictions = theano.function(inputs = [X], outputs = y_vals)\n",
    "        self.debug = theano.function(inputs = [X, Y], outputs = [X.shape, Y.shape, h_vals.shape, y_vals.shape])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Rnn(7, 50, 7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([100,   7]), array([100,   7]), array([100,  50]), array([100,   1,   7])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.14284763,  0.14250721,  0.14240675,  0.14396988,  0.14260595,\n",
       "          0.14442767,  0.14123492]],\n",
       "\n",
       "       [[ 0.14559752,  0.14280474,  0.14049399,  0.14519536,  0.14013915,\n",
       "          0.14428471,  0.14148448]],\n",
       "\n",
       "       [[ 0.14008938,  0.14464262,  0.14370957,  0.14081264,  0.14415146,\n",
       "          0.14370921,  0.14288515]],\n",
       "\n",
       "       [[ 0.14818682,  0.13951913,  0.14199056,  0.14270474,  0.14128901,\n",
       "          0.14287454,  0.14343517]],\n",
       "\n",
       "       [[ 0.14399292,  0.14488365,  0.1453148 ,  0.14258461,  0.13886179,\n",
       "          0.13955611,  0.14480619]],\n",
       "\n",
       "       [[ 0.14164145,  0.14122082,  0.14076631,  0.14671937,  0.14178421,\n",
       "          0.14381069,  0.14405715]],\n",
       "\n",
       "       [[ 0.13907513,  0.14299788,  0.14283548,  0.14237292,  0.14659183,\n",
       "          0.1456562 ,  0.14047052]],\n",
       "\n",
       "       [[ 0.14139324,  0.14561172,  0.14414689,  0.14105491,  0.14538541,\n",
       "          0.14147893,  0.14092895]],\n",
       "\n",
       "       [[ 0.14625072,  0.13952824,  0.13899292,  0.14481378,  0.14001323,\n",
       "          0.14398773,  0.14641342]],\n",
       "\n",
       "       [[ 0.13799566,  0.14431819,  0.14838333,  0.13881923,  0.14530788,\n",
       "          0.14284378,  0.142332  ]],\n",
       "\n",
       "       [[ 0.14133976,  0.14606258,  0.14388682,  0.14136072,  0.14643635,\n",
       "          0.1414216 ,  0.1394922 ]],\n",
       "\n",
       "       [[ 0.14122936,  0.14401223,  0.14197353,  0.1412963 ,  0.14668986,\n",
       "          0.14102432,  0.14377429]],\n",
       "\n",
       "       [[ 0.14358881,  0.14256091,  0.14288609,  0.14502731,  0.1414426 ,\n",
       "          0.14032201,  0.14417225]],\n",
       "\n",
       "       [[ 0.14086972,  0.1440957 ,  0.14331205,  0.14441571,  0.14144565,\n",
       "          0.14236444,  0.14349672]],\n",
       "\n",
       "       [[ 0.14575325,  0.14224523,  0.14220856,  0.14237969,  0.14134917,\n",
       "          0.1434494 ,  0.14261474]],\n",
       "\n",
       "       [[ 0.14336309,  0.14065228,  0.13980487,  0.14555821,  0.14413729,\n",
       "          0.14447759,  0.1420067 ]],\n",
       "\n",
       "       [[ 0.14282042,  0.14446948,  0.14455262,  0.13911068,  0.14391533,\n",
       "          0.14228573,  0.14284582]],\n",
       "\n",
       "       [[ 0.14537475,  0.14112051,  0.14293094,  0.14435785,  0.14133568,\n",
       "          0.14244501,  0.14243527]],\n",
       "\n",
       "       [[ 0.13841219,  0.14321198,  0.14086729,  0.14158353,  0.14622599,\n",
       "          0.14430533,  0.14539373]],\n",
       "\n",
       "       [[ 0.138739  ,  0.14316453,  0.14262411,  0.14194256,  0.14815611,\n",
       "          0.14408568,  0.14128806]],\n",
       "\n",
       "       [[ 0.14630198,  0.14353611,  0.14126147,  0.1421686 ,  0.14227451,\n",
       "          0.14350812,  0.14094917]],\n",
       "\n",
       "       [[ 0.14118324,  0.1441682 ,  0.14389703,  0.14144856,  0.14461978,\n",
       "          0.14065397,  0.14402913]],\n",
       "\n",
       "       [[ 0.14780118,  0.13974629,  0.14135964,  0.14493717,  0.13964409,\n",
       "          0.14149395,  0.14501767]],\n",
       "\n",
       "       [[ 0.13971141,  0.14311156,  0.14279827,  0.14627418,  0.14323395,\n",
       "          0.14448395,  0.1403867 ]],\n",
       "\n",
       "       [[ 0.14266244,  0.14466645,  0.14239955,  0.14075775,  0.14390898,\n",
       "          0.14200892,  0.14359593]],\n",
       "\n",
       "       [[ 0.14031512,  0.14047009,  0.14459459,  0.14269128,  0.14552306,\n",
       "          0.1430006 ,  0.14340526]],\n",
       "\n",
       "       [[ 0.14142197,  0.1437797 ,  0.14124858,  0.14413144,  0.14391403,\n",
       "          0.14231509,  0.14318912]],\n",
       "\n",
       "       [[ 0.14583786,  0.1400196 ,  0.14099097,  0.14386968,  0.14195898,\n",
       "          0.1425309 ,  0.14479205]],\n",
       "\n",
       "       [[ 0.14297737,  0.14369854,  0.14206679,  0.1442671 ,  0.13991135,\n",
       "          0.14296541,  0.14411342]],\n",
       "\n",
       "       [[ 0.14310199,  0.14118241,  0.14620003,  0.14246842,  0.14216703,\n",
       "          0.14362457,  0.14125553]],\n",
       "\n",
       "       [[ 0.14375331,  0.1437256 ,  0.14420377,  0.14312017,  0.1422382 ,\n",
       "          0.14080536,  0.14215352]],\n",
       "\n",
       "       [[ 0.13889676,  0.14199293,  0.13980769,  0.14517909,  0.14469951,\n",
       "          0.14354776,  0.14587621]],\n",
       "\n",
       "       [[ 0.13885596,  0.14194681,  0.14228369,  0.14411384,  0.1450408 ,\n",
       "          0.14290202,  0.144857  ]],\n",
       "\n",
       "       [[ 0.14287634,  0.1429543 ,  0.14297418,  0.14371742,  0.14236133,\n",
       "          0.14387165,  0.14124469]],\n",
       "\n",
       "       [[ 0.1484572 ,  0.14327219,  0.14074609,  0.14256534,  0.13892116,\n",
       "          0.14170094,  0.14433703]],\n",
       "\n",
       "       [[ 0.14392872,  0.14126824,  0.14237733,  0.14354362,  0.14281276,\n",
       "          0.144458  ,  0.14161134]],\n",
       "\n",
       "       [[ 0.1441146 ,  0.14504752,  0.14439036,  0.14087236,  0.14262627,\n",
       "          0.14434423,  0.13860476]],\n",
       "\n",
       "       [[ 0.14353646,  0.14148779,  0.14238779,  0.14384161,  0.14508609,\n",
       "          0.14132093,  0.14233932]],\n",
       "\n",
       "       [[ 0.14291821,  0.14233623,  0.14033121,  0.14501603,  0.14098482,\n",
       "          0.14567687,  0.14273661]],\n",
       "\n",
       "       [[ 0.14246637,  0.143223  ,  0.14351252,  0.14307739,  0.14343213,\n",
       "          0.14320552,  0.14108312]],\n",
       "\n",
       "       [[ 0.13685481,  0.14542413,  0.14610647,  0.13857493,  0.14933249,\n",
       "          0.14317283,  0.14053443]],\n",
       "\n",
       "       [[ 0.14144242,  0.14391674,  0.14176971,  0.14292893,  0.14777634,\n",
       "          0.14020963,  0.1419562 ]],\n",
       "\n",
       "       [[ 0.13894373,  0.14454532,  0.14103958,  0.14432567,  0.14411801,\n",
       "          0.14123096,  0.14579675]],\n",
       "\n",
       "       [[ 0.1459689 ,  0.13932475,  0.14302044,  0.14287682,  0.14089671,\n",
       "          0.14436193,  0.14355041]],\n",
       "\n",
       "       [[ 0.14368148,  0.14362283,  0.14385651,  0.14326556,  0.14101654,\n",
       "          0.14103904,  0.14351803]],\n",
       "\n",
       "       [[ 0.14030719,  0.14294501,  0.14648515,  0.14397505,  0.14430249,\n",
       "          0.14213847,  0.1398467 ]],\n",
       "\n",
       "       [[ 0.14205608,  0.14266501,  0.14024937,  0.145429  ,  0.14388058,\n",
       "          0.14530504,  0.14041482]],\n",
       "\n",
       "       [[ 0.14152141,  0.14513335,  0.1410661 ,  0.14230753,  0.14211282,\n",
       "          0.1453478 ,  0.14251104]],\n",
       "\n",
       "       [[ 0.14160316,  0.14413749,  0.14567624,  0.13971359,  0.14354979,\n",
       "          0.14207943,  0.14324035]],\n",
       "\n",
       "       [[ 0.1475157 ,  0.13953024,  0.13846277,  0.1467621 ,  0.13943253,\n",
       "          0.14627029,  0.14202642]],\n",
       "\n",
       "       [[ 0.14274871,  0.14415571,  0.14699529,  0.14022942,  0.14369795,\n",
       "          0.140746  ,  0.14142701]],\n",
       "\n",
       "       [[ 0.1441727 ,  0.14422679,  0.14255939,  0.14433348,  0.13989073,\n",
       "          0.14141546,  0.1434014 ]],\n",
       "\n",
       "       [[ 0.13812658,  0.14184904,  0.14351392,  0.14270653,  0.14599332,\n",
       "          0.14358126,  0.1442294 ]],\n",
       "\n",
       "       [[ 0.14011255,  0.14439227,  0.14297402,  0.14324863,  0.14542769,\n",
       "          0.14465752,  0.13918737]],\n",
       "\n",
       "       [[ 0.14399584,  0.14476235,  0.14423352,  0.13823986,  0.14290468,\n",
       "          0.14116806,  0.14469571]],\n",
       "\n",
       "       [[ 0.14073728,  0.14262244,  0.14458153,  0.14186099,  0.14425042,\n",
       "          0.14217468,  0.14377266]],\n",
       "\n",
       "       [[ 0.1429745 ,  0.14594518,  0.14293832,  0.14092696,  0.14354645,\n",
       "          0.14254038,  0.1411282 ]],\n",
       "\n",
       "       [[ 0.14571451,  0.14332052,  0.14275156,  0.14168674,  0.14248325,\n",
       "          0.14104427,  0.14299919]],\n",
       "\n",
       "       [[ 0.14503485,  0.14298625,  0.14299318,  0.14332506,  0.1381954 ,\n",
       "          0.14322004,  0.14424522]],\n",
       "\n",
       "       [[ 0.14426167,  0.13964488,  0.14248979,  0.14390071,  0.14282754,\n",
       "          0.14480671,  0.14206868]],\n",
       "\n",
       "       [[ 0.14106597,  0.14515066,  0.14509   ,  0.14076926,  0.14553462,\n",
       "          0.14095287,  0.1414367 ]],\n",
       "\n",
       "       [[ 0.14403087,  0.14474887,  0.14511199,  0.14035136,  0.14168771,\n",
       "          0.13780281,  0.14626645]],\n",
       "\n",
       "       [[ 0.14348149,  0.13970338,  0.14428633,  0.14638329,  0.14151879,\n",
       "          0.14237075,  0.14225602]],\n",
       "\n",
       "       [[ 0.14201845,  0.1437009 ,  0.14139208,  0.14274596,  0.1444771 ,\n",
       "          0.14369334,  0.14197215]],\n",
       "\n",
       "       [[ 0.14359055,  0.14315026,  0.14732768,  0.14038256,  0.14448211,\n",
       "          0.14034614,  0.14072071]],\n",
       "\n",
       "       [[ 0.14333811,  0.14343743,  0.14268681,  0.14337628,  0.14158769,\n",
       "          0.14144823,  0.14412546]],\n",
       "\n",
       "       [[ 0.14477178,  0.14047281,  0.14155543,  0.14510816,  0.14151442,\n",
       "          0.14462726,  0.14195013]],\n",
       "\n",
       "       [[ 0.14549303,  0.14302431,  0.14260641,  0.14397024,  0.14114255,\n",
       "          0.14072755,  0.14303586]],\n",
       "\n",
       "       [[ 0.14535896,  0.14187735,  0.14306544,  0.14469363,  0.14042839,\n",
       "          0.14186125,  0.14271501]],\n",
       "\n",
       "       [[ 0.14014098,  0.14201039,  0.14252156,  0.14496763,  0.14585789,\n",
       "          0.14275604,  0.14174552]],\n",
       "\n",
       "       [[ 0.14983931,  0.14260206,  0.14301197,  0.14396605,  0.13680154,\n",
       "          0.14047471,  0.14330435]],\n",
       "\n",
       "       [[ 0.14133434,  0.13933751,  0.14378518,  0.14366494,  0.14401563,\n",
       "          0.14605594,  0.14180651]],\n",
       "\n",
       "       [[ 0.14816341,  0.1453162 ,  0.14457427,  0.13959952,  0.14001349,\n",
       "          0.14115712,  0.14117597]],\n",
       "\n",
       "       [[ 0.13986005,  0.14207074,  0.14239158,  0.14506197,  0.14449383,\n",
       "          0.14147574,  0.14464611]],\n",
       "\n",
       "       [[ 0.14165653,  0.14352173,  0.14283699,  0.14144029,  0.14289066,\n",
       "          0.14415373,  0.14350012]],\n",
       "\n",
       "       [[ 0.14478956,  0.13923283,  0.14137563,  0.14530927,  0.14259787,\n",
       "          0.14411151,  0.14258336]],\n",
       "\n",
       "       [[ 0.14267384,  0.14548939,  0.14457417,  0.14012744,  0.14401187,\n",
       "          0.13928252,  0.14384077]],\n",
       "\n",
       "       [[ 0.13902594,  0.1444786 ,  0.14273646,  0.14342801,  0.14612077,\n",
       "          0.14282048,  0.14138973]],\n",
       "\n",
       "       [[ 0.14178921,  0.14238642,  0.14305392,  0.14152397,  0.1459823 ,\n",
       "          0.14509979,  0.14016443]],\n",
       "\n",
       "       [[ 0.14394489,  0.14638978,  0.14465949,  0.13926677,  0.14227872,\n",
       "          0.14092709,  0.14253329]],\n",
       "\n",
       "       [[ 0.14465387,  0.14198518,  0.14196515,  0.14184384,  0.14300649,\n",
       "          0.14038989,  0.14615564]],\n",
       "\n",
       "       [[ 0.14402407,  0.14157882,  0.14162475,  0.14660409,  0.14069949,\n",
       "          0.14545035,  0.14001842]],\n",
       "\n",
       "       [[ 0.14277753,  0.14521168,  0.14452316,  0.1400294 ,  0.14284059,\n",
       "          0.1423272 ,  0.14229046]],\n",
       "\n",
       "       [[ 0.14725007,  0.14056295,  0.14515206,  0.14291474,  0.13930513,\n",
       "          0.14214563,  0.14266944]],\n",
       "\n",
       "       [[ 0.14692856,  0.14176247,  0.14078218,  0.14556994,  0.13853607,\n",
       "          0.14320587,  0.14321494]],\n",
       "\n",
       "       [[ 0.14638871,  0.14497171,  0.14557588,  0.14113089,  0.13917835,\n",
       "          0.14019284,  0.14256166]],\n",
       "\n",
       "       [[ 0.14400506,  0.14110446,  0.14383385,  0.14431794,  0.14145045,\n",
       "          0.14685088,  0.13843733]],\n",
       "\n",
       "       [[ 0.14621279,  0.14362204,  0.14085987,  0.14451559,  0.1406077 ,\n",
       "          0.14168742,  0.14249453]],\n",
       "\n",
       "       [[ 0.14249232,  0.14242463,  0.14392763,  0.14253847,  0.14027551,\n",
       "          0.14650108,  0.1418404 ]],\n",
       "\n",
       "       [[ 0.14153451,  0.1424481 ,  0.14130856,  0.14273575,  0.14408565,\n",
       "          0.14224166,  0.14564577]],\n",
       "\n",
       "       [[ 0.14028543,  0.14149082,  0.14059219,  0.14311931,  0.14572008,\n",
       "          0.1448689 ,  0.14392328]],\n",
       "\n",
       "       [[ 0.14391854,  0.14466228,  0.14244823,  0.14168398,  0.14206776,\n",
       "          0.13972589,  0.14549333]],\n",
       "\n",
       "       [[ 0.14687091,  0.1393045 ,  0.13931552,  0.14818662,  0.137007  ,\n",
       "          0.14298503,  0.14633043]],\n",
       "\n",
       "       [[ 0.14054357,  0.14123729,  0.14318533,  0.14268783,  0.14474946,\n",
       "          0.14459848,  0.14299795]],\n",
       "\n",
       "       [[ 0.1474531 ,  0.14376596,  0.1429361 ,  0.14270093,  0.13985251,\n",
       "          0.14323735,  0.14005412]],\n",
       "\n",
       "       [[ 0.14466546,  0.14148018,  0.14602156,  0.14061345,  0.14244293,\n",
       "          0.14125881,  0.14351769]],\n",
       "\n",
       "       [[ 0.14464682,  0.14264774,  0.14366081,  0.14303361,  0.14000055,\n",
       "          0.14193614,  0.14407428]],\n",
       "\n",
       "       [[ 0.14550138,  0.14111573,  0.14315121,  0.14509526,  0.14074907,\n",
       "          0.13974753,  0.14463991]],\n",
       "\n",
       "       [[ 0.14394118,  0.1414265 ,  0.14195935,  0.14553301,  0.13999791,\n",
       "          0.14345635,  0.14368562]],\n",
       "\n",
       "       [[ 0.14311695,  0.14219461,  0.14697242,  0.13962375,  0.14319678,\n",
       "          0.14319448,  0.14170109]]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sequences of 100 elements and vector size 7\n",
    "X = np.random.uniform(low=-0.1, high=0.1, size=(100,7)).astype(dtype=dtype) \n",
    "Y = np.random.uniform(low=-0.1, high=0.1, size=(100,7)).astype(dtype=dtype)\n",
    "\n",
    "print(model.debug(X,Y))\n",
    "model.predictions(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.15975379943847656, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6d16d2186e60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "#stupid and naive sgd\n",
    "for x in range(nb_epochs):\n",
    "    error = 0.\n",
    "    for j in range(len(train_data)):  \n",
    "        index = np.random.randint(0, len(train_data))\n",
    "        i, o = train_data[index]\n",
    "        train_cost = model.train(i, o)\n",
    "        error += train_cost\n",
    "    if x%10==0:\n",
    "            print \"epoch \"+str(x)+ \" error: \"+str(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
